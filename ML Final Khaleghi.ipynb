{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project\n",
    "## Sentiment analysis of hotel reviews using neural networks\n",
    "\n",
    "In this project, I build upon the work in my capstone project. There, we used sklearn's TF-IDF vectorizer to encode ngrams of words and use them to classify a review as a score from 1-5, to include in that recommender system model. Using a lot of computing power, I was able to achieve significant accuracy using that method on the entire set of reviews. However, as noted in detail below, sampling the reviews had far less accuracy.\n",
    "\n",
    "In this case, we are looking at the sentiment analysis of the text with the goal to build a neural network model that is better than the bag of words model. The secondary goal is get pratice building different types of neural networks and understand the tuning of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I define a function to pull the text reviews (and other data) into a dataframe from the JSON format, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to turn the json files into a dataframe\n",
    "def json_to_df(filename):\n",
    "    \"\"\"Return a Pandas DataFrame with Reviews from the JSON File given.\"\"\"\n",
    "\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "        reviews = data['Reviews']\n",
    "        info = data['HotelInfo']\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for review in reviews:\n",
    "        rr = pd.Series(review['Ratings'], name=review['ReviewID'])\n",
    "        rr['Date'] = review['Date']\n",
    "        rr['Author'] = review['Author']\n",
    "        try:\n",
    "            rr['AuthorLocation'] = review['AuthorLocation'].split(', ')[-1]\n",
    "        except Exception:\n",
    "            rr['AuthorLocation'] = np.nan\n",
    "        rr['Review'] = review['Content']\n",
    "        df = df.append(rr)\n",
    "\n",
    "\n",
    "    # Hotel Info\n",
    "    df['HotelID'] = int(info['HotelID'])\n",
    "    try:\n",
    "        df['Hotel'] = info['Name']\n",
    "    except Exception:\n",
    "        df['Hotel'] = \"\"\n",
    "    price_range = [int(''.join([el for el in price if el.isdigit()])) for price in info['Price'].split('-')]\n",
    "    df['PriceMin'] = price_range[0]\n",
    "    df['PriceMax'] = price_range[-1]\n",
    "    try:\n",
    "        address = BeautifulSoup(info['Address'], 'lxml')\n",
    "    except Exception:\n",
    "        address = \"\"\n",
    "    try:\n",
    "        region = address.find('span', property='v:region').text\n",
    "    except Exception:\n",
    "        region = np.nan\n",
    "    try:\n",
    "        df['HotelLocation'] = region\n",
    "    except Exception:\n",
    "        df['HotelLocation'] = \"\"\n",
    "         \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the function defined and some error catching thrown in, as the reviews are often missing some data, we begin the process of sampling and loading the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1910036.json... done  - 1\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/484017.json... done  - 2\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/614389.json... done  - 3\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/305486.json... done  - 4\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/841981.json... done  - 5\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/589554.json... done  - 6\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/73855.json... done  - 7\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/294930.json... done  - 8\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/234947.json... done  - 9\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/636152.json... done  - 10\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1223953.json... done  - 11\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/282608.json... done  - 12\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/223273.json... done  - 13\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/290385.json... done  - 14\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/587302.json... done  - 15\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1005302.json... done  - 16\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/259229.json... done  - 17\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/233568.json... done  - 18\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515039.json... done  - 19\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/88134.json... done  - 20\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/324563.json... done  - 21\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/659080.json... done  - 22\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515952.json... done  - 23\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1235711.json... done  - 24\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/228858.json... done  - 25\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/233408.json... done  - 26\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/633316.json... done  - 27\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/77787.json... done  - 28\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/671326.json... done  - 29\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/224732.json... done  - 30\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/268635.json... done  - 31\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/93475.json... done  - 32\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/231629.json... done  - 33\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/98693.json... done  - 34\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1022712.json... done  - 35\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/224852.json... done  - 36\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/307571.json... done  - 37\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514839.json... done  - 38\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/84804.json... done  - 39\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/529514.json... done  - 40\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514968.json... done  - 41\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515654.json... done  - 42\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/235216.json... done  - 43\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515022.json... done  - 44\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2516018.json... done  - 45\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/314854.json... done  - 46\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515248.json... done  - 47\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/664663.json... done  - 48\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/259313.json... done  - 49\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/235994.json... done  - 50\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/338304.json... done  - 51\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/309419.json... done  - 52\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/529336.json... done  - 53\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/312451.json... done  - 54\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514996.json... done  - 55\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/656462.json... done  - 56\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/651639.json... done  - 57\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/87006.json... done  - 58\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/202976.json... done  - 59\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/235826.json... done  - 60\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514437.json... done  - 61\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/551172.json... done  - 62\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2013911.json... done  - 63\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/946694.json... done  - 64\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515628.json... done  - 65\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514867.json... done  - 66\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/102537.json... done  - 67\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1228907.json... done  - 68\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/229006.json... done  - 69\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/664220.json... done  - 70\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/230551.json... done  - 71\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/228938.json... done  - 72\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/623542.json... done  - 73\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/231527.json... done  - 74\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/290554.json... done  - 75\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1218038.json... done  - 76\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/240154.json... done  - 77\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/545850.json... done  - 78\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1141159.json... done  - 79\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1006211.json... done  - 80\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/277143.json... done  - 81\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515777.json... done  - 82\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/260636.json... done  - 83\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515645.json... done  - 84\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/229686.json... done  - 85\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/620399.json... done  - 86\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1237001.json... done  - 87\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/259355.json... done  - 88\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/96715.json... done  - 89\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/296496.json... done  - 90\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/596010.json... done  - 91\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/665258.json... done  - 92\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514507.json... done  - 93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/73884.json... done  - 94\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514827.json... done  - 95\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/91607.json... done  - 96\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/281234.json... done  - 97\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/268433.json... done  - 98\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1174552.json... done  - 99\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/507538.json... done  - 100\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/678622.json... done  - 101\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/99365.json... done  - 102\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/230355.json... done  - 103\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/78135.json... done  - 104\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/580663.json... done  - 105\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/202998.json... done  - 106\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1176843.json... done  - 107\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/228626.json... done  - 108\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514351.json... done  - 109\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/217626.json... done  - 110\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/119479.json... done  - 111\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/237522.json... done  - 112\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/611947.json... done  - 113\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1146432.json... done  - 114\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515943.json... done  - 115\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/282723.json... done  - 116\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/306158.json... done  - 117\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/590139.json... done  - 118\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/275098.json... done  - 119\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/114032.json... done  - 120\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1093097.json... done  - 121\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/264641.json... done  - 122\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/224196.json... done  - 123\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/236157.json... done  - 124\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/603085.json... done  - 125\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/619153.json... done  - 126\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/193669.json... done  - 127\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/603168.json... done  - 128\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/553834.json... done  - 129\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/229080.json... done  - 130\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/76460.json... done  - 131\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/652607.json... done  - 132\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1007348.json... done  - 133\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/87039.json... done  - 134\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/623066.json... done  - 135\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515343.json... done  - 136\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/121998.json... done  - 137\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1590839.json... done  - 138\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/224858.json... done  - 139\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/596886.json... done  - 140\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2180685.json... done  - 141\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515650.json... done  - 142\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/192103.json... done  - 143\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/729701.json... done  - 144\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/535409.json... done  - 145\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2202220.json... done  - 146\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1413478.json... done  - 147\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/674283.json... done  - 148\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/218479.json... done  - 149\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2140201.json... done  - 150\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/305671.json... done  - 151\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/193115.json... done  - 152\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1233228.json... done  - 153\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1156822.json... done  - 154\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/289464.json... done  - 155\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/123037.json... done  - 156\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/232223.json... done  - 157\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1307596.json... done  - 158\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/87107.json... done  - 159\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/189699.json... done  - 160\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514848.json... done  - 161\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/776335.json... done  - 162\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/120210.json... done  - 163\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/233737.json... done  - 164\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1084013.json... done  - 165\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/529193.json... done  - 166\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2516090.json... done  - 167\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/306197.json... done  - 168\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/154645.json... done  - 169\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/89143.json... done  - 170\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/261990.json... done  - 171\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/482631.json... done  - 172\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/229105.json... done  - 173\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515243.json... done  - 174\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/596670.json... done  - 175\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/574958.json... done  - 176\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/239867.json... done  - 177\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/263608.json... done  - 178\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/90767.json... done  - 179\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/302401.json... done  - 180\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514260.json... done  - 181\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/81522.json... done  - 182\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/230875.json... done  - 183\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/77247.json... done  - 184\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/622666.json... done  - 185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515536.json... done  - 186\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/85007.json... done  - 187\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/582157.json... done  - 188\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/89357.json... done  - 189\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1196040.json... done  - 190\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/232321.json... done  - 191\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/1434751.json... done  - 192\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/229441.json... done  - 193\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2514871.json... done  - 194\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/642994.json... done  - 195\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/227457.json... done  - 196\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2515909.json... done  - 197\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/634557.json... done  - 198\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/2516025.json... done  - 199\n",
      "/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json/99425.json... done  - 200\n",
      "                 Author  AuthorLocation Business service  \\\n",
      "UR126450078    ruckus87        New York              NaN   \n",
      "UR126433804  ThamyresVM  United Kingdom              NaN   \n",
      "\n",
      "            Business service (e.g., internet access) Check in / front desk  \\\n",
      "UR126450078                                      NaN                   NaN   \n",
      "UR126433804                                      NaN                   NaN   \n",
      "\n",
      "            Cleanliness            Date                    Hotel  HotelID  \\\n",
      "UR126450078           5  March 21, 2012  Hotel La Belle Juliette  1910036   \n",
      "UR126433804           5  March 21, 2012  Hotel La Belle Juliette  1910036   \n",
      "\n",
      "            HotelLocation Location Overall  PriceMax  PriceMin  \\\n",
      "UR126450078           NaN        4     5.0       885       287   \n",
      "UR126433804           NaN        5     5.0       885       287   \n",
      "\n",
      "                                                        Review Rooms Service  \\\n",
      "UR126450078  My wife and I recently stayed here for 5 night...     4       5   \n",
      "UR126433804  Great location, friendly staff, amazing rooms ...     5       5   \n",
      "\n",
      "            Sleep Quality Value  \n",
      "UR126450078             5     5  \n",
      "UR126433804             4     4  \n",
      "284.0499926120974\n",
      "24625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ryan/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:27: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer #for timing\n",
    "\n",
    "start = timer()\n",
    "#load data into dataframe\n",
    "path ='/Users/Ryan/Desktop/Programming/SMU/Capstone/TripAdvisor/json' \n",
    "allFiles = glob.glob(path + \"/*.json\")\n",
    "#print(allFiles)\n",
    "#due to computing constraints, we have to sample the data set and can't use the whole thing\n",
    "#so we randomly sample some number of them\n",
    "#1000 hotels takes over half an hour to load on my laptop\n",
    "randFiles = np.random.choice(allFiles, 200, replace=False)\n",
    "df2 = pd.DataFrame()\n",
    "df_reviews = pd.DataFrame()\n",
    "list_ = []\n",
    "i = 1\n",
    "\n",
    "#switch between either sampling or all of the files\n",
    "for file_ in randFiles:\n",
    "#for file_ in allFiles:\n",
    "    try:\n",
    "        #df = tripadvisor_convert.to_df(file_)\n",
    "        df2 = json_to_df(file_)\n",
    "    #df2 = json_to_df(file_)\n",
    "    except Exception:\n",
    "        pass \n",
    "    list_.append(df2)\n",
    "    df_reviews = pd.concat(list_)\n",
    "    print(file_ + \"... done  -\", i)\n",
    "    i = i+1\n",
    "end = timer()\n",
    "\n",
    "df_reviews.drop_duplicates(keep='first', inplace=True)\n",
    "print(df_reviews.head(2))\n",
    "print (end - start)\n",
    "print(len(df_reviews))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, since I don't need anything at this point but the overall score and the review text, we drop the rest. I am also ignoring EDA, since this has been done already on the Capstone/Thesis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UR126450078</th>\n",
       "      <td>5.0</td>\n",
       "      <td>My wife and I recently stayed here for 5 night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UR126433804</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Great location, friendly staff, amazing rooms ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UR126126502</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Ok, this hotel looks great, the interior decor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UR125766797</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This really could be a one word review, with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UR125671278</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Stayed at La Belle Juliette for 4 nights. It i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Overall                                             Review\n",
       "UR126450078     5.0  My wife and I recently stayed here for 5 night...\n",
       "UR126433804     5.0  Great location, friendly staff, amazing rooms ...\n",
       "UR126126502     3.0  Ok, this hotel looks great, the interior decor...\n",
       "UR125766797     5.0  This really could be a one word review, with t...\n",
       "UR125671278     5.0  Stayed at La Belle Juliette for 4 nights. It i..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop extra columns\n",
    "df_reviews = df_reviews.filter(items=['Overall', 'Review'])\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some method must be now used to classify each review text, as written, as either positive (1) or negative (0). Many people use a manual process with humans doing the scoring, but this isn't doable here. I chose to infer the classification for the Overall rating for the hotel that accompanies the review. If the Overall rating for the hotel is 3 or higher, I am assuming the written review is positive. If it's 1 or 2, I'm assuming it is negative.\n",
    "\n",
    "I noted in the capstone project that this isn't an ideal method. In some instances, the review text and Overall rating don't match up. A paper I referenced backs this up, that some times there isn't strong correlation between the review text and the scores. It will hopefully be good enough for our purposes since it is true in most cases, and it seems like common sense that this would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UR126450078</th>\n",
       "      <td>5.0</td>\n",
       "      <td>My wife and I recently stayed here for 5 night...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UR126433804</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Great location, friendly staff, amazing rooms ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Overall                                             Review  \\\n",
       "UR126450078     5.0  My wife and I recently stayed here for 5 night...   \n",
       "UR126433804     5.0  Great location, friendly staff, amazing rooms ...   \n",
       "\n",
       "             sentiment  \n",
       "UR126450078          1  \n",
       "UR126433804          1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is lazy but i cant classify tens or hundreds of thousands of reviews manually\n",
    "#this is binary and necessary for the neural networks\n",
    "#add to df\n",
    "\n",
    "sentiment = []\n",
    "\n",
    "for row in df_reviews['Overall']:\n",
    "    if float(row) >= 3:\n",
    "        sentiment.append(1)\n",
    "    else:\n",
    "        sentiment.append(0)\n",
    "        \n",
    "df_reviews['sentiment'] = sentiment\n",
    "\n",
    "df_reviews.head(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the reviews classified as positive and negative in the \"sentiment\" column, we can drop the Overall rating of the hotels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UR126450078</th>\n",
       "      <td>5.0</td>\n",
       "      <td>My wife and I recently stayed here for 5 night...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UR126433804</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Great location, friendly staff, amazing rooms ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Overall                                             Review  \\\n",
       "UR126450078     5.0  My wife and I recently stayed here for 5 night...   \n",
       "UR126433804     5.0  Great location, friendly staff, amazing rooms ...   \n",
       "\n",
       "             sentiment  \n",
       "UR126450078          1  \n",
       "UR126433804          1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop overall scores\n",
    "df_reviews.drop(columns=['Overall'])\n",
    "df_reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we make the models, we do a 80/20% train test split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19700,) (4925,) (19700,) (4925,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_reviews['Review'],df_reviews['sentiment'], test_size=0.2,random_state=42,stratify=df_reviews['sentiment'])\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer\n",
    "\n",
    "From my capstone project, I did this analysis to classify text reviews as a score from 1-5. In that case, using the entire corpus of 1.6+ million reviews, we achieved a 95% accuracy. However, this required using Amazon Web Services, and isn't doable for this project. Instead, using the sample method above, I only achieved 60-65% accuracy using between 100 and 1000 hotels worth of reviews. This is a significant decrease in accuracy compared to using the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:18.044872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ryan/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime\n",
    "\n",
    "#uses groups of 1 or 2 words\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "t1 = datetime.datetime.now()\n",
    "vectors = vectorizer.fit_transform(df_reviews['Review'])\n",
    "print(datetime.datetime.now()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train split on this data using vectors df\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(vectors, df_reviews['sentiment'], test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.972765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "classifier = LinearSVC()\n",
    "\n",
    "t1 = datetime.datetime.now()\n",
    "classifier.fit(X_train2, y_train2)\n",
    "print(datetime.datetime.now()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds = classifier.predict(X_test2)\n",
    "print (list(preds[:10]))\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9476142131979696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print (\"Accuracy Score: \", accuracy_score(y_test2, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy achieved here is much higher than expected. As above, previous use of this method by classifying on a score from 1-5 resulted in only a .60 -.65 accuracy. For this sentiment analysis problem, the accuracy is .95. That is a high bar to beat for the deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model, Sequential, load_model\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting this to 100,000 words\n",
    "MAX_WORDS = 100000\n",
    "\n",
    "#initialize tokenizer and fit to review texts\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(df_reviews['Review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tokenizer to train and test sets\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19700, 200) (4925, 200)\n"
     ]
    }
   ],
   "source": [
    "#pad length to make them all the same length, as required for RNN\n",
    "MAX_LENGTH = 200\n",
    "#increasing the max length dramatically increases processing time\n",
    "padded_train = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
    "padded_test = pad_sequences(test_sequences, maxlen=MAX_LENGTH)\n",
    "\n",
    "print(padded_train.shape, padded_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn model\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.random.random((MAX_WORDS, embedding_dim))\n",
    "\n",
    "inp = Input(shape=(MAX_LENGTH, ))\n",
    "x = Embedding(input_dim=MAX_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, weights=[embedding_matrix],trainable=True)(inp)\n",
    "x = SpatialDropout1D(0.1)(x)\n",
    "x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "conc = concatenate([avg_pool, max_pool])\n",
    "outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "rnn_simple_model = Model(inputs = inp, outputs=outp)\n",
    "rnn_simple_model.compile(loss=\"binary_crossentropy\", optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19700 19700 4925 4925\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_train), len(y_train), len(padded_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19700 samples, validate on 4925 samples\n",
      "Epoch 1/10\n",
      "19700/19700 [==============================] - 286s 15ms/step - loss: 0.1091 - acc: 0.9584 - val_loss: 0.1515 - val_acc: 0.9411\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.93868 to 0.94112, saving model to /Users/Ryan/Desktop/Programming/SMU/Machine Learning/weights-improvement-01-0.9411.hdf5\n",
      "Epoch 2/10\n",
      "19700/19700 [==============================] - 287s 15ms/step - loss: 0.0854 - acc: 0.9718 - val_loss: 0.1517 - val_acc: 0.9409\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.94112\n",
      "Epoch 3/10\n",
      "19700/19700 [==============================] - 285s 14ms/step - loss: 0.0696 - acc: 0.9783 - val_loss: 0.1746 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.94112\n",
      "Epoch 4/10\n",
      "19700/19700 [==============================] - 287s 15ms/step - loss: 0.0554 - acc: 0.9852 - val_loss: 0.1543 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.94112\n",
      "Epoch 5/10\n",
      "19700/19700 [==============================] - 285s 14ms/step - loss: 0.0437 - acc: 0.9894 - val_loss: 0.1766 - val_acc: 0.9358\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.94112\n",
      "Epoch 6/10\n",
      "19700/19700 [==============================] - 285s 14ms/step - loss: 0.0354 - acc: 0.9922 - val_loss: 0.1589 - val_acc: 0.9413\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.94112 to 0.94132, saving model to /Users/Ryan/Desktop/Programming/SMU/Machine Learning/weights-improvement-06-0.9413.hdf5\n",
      "Epoch 7/10\n",
      "19700/19700 [==============================] - 285s 14ms/step - loss: 0.0250 - acc: 0.9959 - val_loss: 0.1681 - val_acc: 0.9419\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.94132 to 0.94193, saving model to /Users/Ryan/Desktop/Programming/SMU/Machine Learning/weights-improvement-07-0.9419.hdf5\n",
      "Epoch 8/10\n",
      "19700/19700 [==============================] - 284s 14ms/step - loss: 0.0204 - acc: 0.9974 - val_loss: 0.1684 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.94193\n",
      "Epoch 9/10\n",
      "19700/19700 [==============================] - 284s 14ms/step - loss: 0.0149 - acc: 0.9988 - val_loss: 0.1713 - val_acc: 0.9387\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.94193\n",
      "Epoch 10/10\n",
      "19700/19700 [==============================] - 284s 14ms/step - loss: 0.0118 - acc: 0.9995 - val_loss: 0.1781 - val_acc: 0.9417\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.94193\n",
      "4925/4925 [==============================] - 18s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "history = rnn_simple_model.fit(x = padded_train, \n",
    "                               y = y_train, \n",
    "                               validation_data=(padded_test, y_test),\n",
    "                               batch_size=256, \n",
    "                               callbacks=[checkpoint], \n",
    "                               epochs=10, #5, 10\n",
    "                               verbose=1)\n",
    "\n",
    "y_pred_rnn_simple = rnn_simple_model.predict(padded_test, verbose=1, batch_size=1024)\n",
    "\n",
    "y_pred_rnn_simple = pd.DataFrame(y_pred_rnn_simple, columns=['prediction'])\n",
    "y_pred_rnn_simple['prediction'] = y_pred_rnn_simple['prediction'].map(lambda p: 1 if p >= 0.5 else 0)\n",
    "y_pred_rnn_simple.to_csv('/Users/Ryan/Desktop/Programming/SMU/Machine Learning/y_pred_rnn_simple.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9417258883248731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "y_pred_rnn_simple = pd.read_csv('/Users/Ryan/Desktop/Programming/SMU/Machine Learning/y_pred_rnn_simple.csv')\n",
    "print(accuracy_score(y_test, y_pred_rnn_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RNN results accuracy for the sample is almost the same as the TF-IDF model, and does very well. \n",
    "\n",
    "We will now try a CNN model. CNN's typically haven't been used for NLP, but recent work suggests they can be very effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 300\n",
    "    \n",
    "filter_sizes = [2, 3, 5]\n",
    "num_filters = 256\n",
    "drop = 0.3\n",
    "\n",
    "inputs = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "embedding = Embedding(input_dim=MAX_WORDS,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=MAX_LENGTH,\n",
    "                        trainable=True)(inputs)\n",
    "\n",
    "reshape = Reshape((MAX_LENGTH, embedding_dim, 1))(embedding)\n",
    "conv_0 = Conv2D(num_filters, \n",
    "                kernel_size=(filter_sizes[0], embedding_dim), \n",
    "                padding='valid', kernel_initializer='normal', \n",
    "                activation='relu')(reshape)\n",
    "\n",
    "conv_1 = Conv2D(num_filters, \n",
    "                kernel_size=(filter_sizes[1], embedding_dim), \n",
    "                padding='valid', kernel_initializer='normal', \n",
    "                activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, \n",
    "                kernel_size=(filter_sizes[2], embedding_dim), \n",
    "                padding='valid', kernel_initializer='normal', \n",
    "                activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), \n",
    "                strides=(1,1), padding='valid')(conv_0)\n",
    "\n",
    "maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), \n",
    "                strides=(1,1), padding='valid')(conv_1)\n",
    "\n",
    "maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), \n",
    "                strides=(1,1), padding='valid')(conv_2)\n",
    "concatenated_tensor = Concatenate(axis=1)(\n",
    "    [maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=1, activation='sigmoid')(dropout)\n",
    "\n",
    "cnn_model_multi_channel = Model(inputs=inputs, outputs=output)\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "cnn_model_multi_channel.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19700 samples, validate on 4925 samples\n",
      "Epoch 1/10\n",
      "19700/19700 [==============================] - 370s 19ms/step - loss: 0.4391 - acc: 0.8635 - val_loss: 0.3621 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.94193\n",
      "Epoch 2/10\n",
      "19700/19700 [==============================] - 365s 19ms/step - loss: 0.3796 - acc: 0.8754 - val_loss: 0.3263 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.94193\n",
      "Epoch 3/10\n",
      "19700/19700 [==============================] - 365s 19ms/step - loss: 0.3405 - acc: 0.8814 - val_loss: 0.3049 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.94193\n",
      "Epoch 4/10\n",
      "19700/19700 [==============================] - 364s 18ms/step - loss: 0.3110 - acc: 0.8837 - val_loss: 0.2866 - val_acc: 0.8835\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.94193\n",
      "Epoch 5/10\n",
      "19700/19700 [==============================] - 368s 19ms/step - loss: 0.2883 - acc: 0.8888 - val_loss: 0.2751 - val_acc: 0.8887\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.94193\n",
      "Epoch 6/10\n",
      "19700/19700 [==============================] - 366s 19ms/step - loss: 0.2653 - acc: 0.8927 - val_loss: 0.2575 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.94193\n",
      "Epoch 7/10\n",
      "19700/19700 [==============================] - 378s 19ms/step - loss: 0.2476 - acc: 0.8972 - val_loss: 0.2466 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.94193\n",
      "Epoch 8/10\n",
      "19700/19700 [==============================] - 371s 19ms/step - loss: 0.2298 - acc: 0.9038 - val_loss: 0.2375 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.94193\n",
      "Epoch 9/10\n",
      "19700/19700 [==============================] - 363s 18ms/step - loss: 0.2183 - acc: 0.9112 - val_loss: 0.2354 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.94193\n",
      "Epoch 10/10\n",
      "19700/19700 [==============================] - 365s 19ms/step - loss: 0.2062 - acc: 0.9150 - val_loss: 0.2211 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.94193\n",
      "4925/4925 [==============================] - 31s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "history = cnn_model_multi_channel.fit(x=padded_train, \n",
    "                    y=y_train, \n",
    "                    validation_data=(padded_test, y_test), \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[checkpoint], \n",
    "                    epochs=epochs, \n",
    "                    verbose=1)\n",
    "\n",
    "y_pred_cnn_multi_channel = cnn_model_multi_channel.predict(padded_test, verbose=1, batch_size=2048)\n",
    "\n",
    "y_pred_cnn_multi_channel = pd.DataFrame(y_pred_cnn_multi_channel, columns=['prediction'])\n",
    "y_pred_cnn_multi_channel['prediction'] = y_pred_cnn_multi_channel['prediction'].map(lambda p: 1 if p >= 0.5 else 0)\n",
    "y_pred_cnn_multi_channel.to_csv('/Users/Ryan/Desktop/Programming/SMU/Machine Learning/y_pred_cnn_multi_channel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9084263959390863\n"
     ]
    }
   ],
   "source": [
    "y_pred_cnn_multi_channel = pd.read_csv('/Users/Ryan/Desktop/Programming/SMU/Machine Learning/y_pred_cnn_multi_channel.csv')\n",
    "print(accuracy_score(y_test, y_pred_cnn_multi_channel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is slightly lower than the RNN model, but still good. This is expected, as research indicates that CNNs aren't typically used for NLP, but this is still a good result.\n",
    "\n",
    "My research also suggests that using a CNN combined with an RNN a better model than a CNN or RNN alone, so I will do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "inp = Input(shape=(MAX_LENGTH, ))\n",
    "x = Embedding(MAX_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=True)(inp)\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "conc = concatenate([avg_pool, max_pool])\n",
    "outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "rnn_cnn_model = Model(inputs=inp, outputs=outp)\n",
    "rnn_cnn_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20879 samples, validate on 5220 samples\n",
      "Epoch 1/4\n",
      "20879/20879 [==============================] - 305s 15ms/step - loss: 0.3984 - acc: 0.8652 - val_loss: 0.3295 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92816\n",
      "Epoch 2/4\n",
      "20879/20879 [==============================] - 300s 14ms/step - loss: 0.3077 - acc: 0.8782 - val_loss: 0.2645 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92816\n",
      "Epoch 3/4\n",
      "20879/20879 [==============================] - 300s 14ms/step - loss: 0.2357 - acc: 0.9029 - val_loss: 0.2029 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92816\n",
      "Epoch 4/4\n",
      "20879/20879 [==============================] - 300s 14ms/step - loss: 0.1919 - acc: 0.9232 - val_loss: 0.1847 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92816\n",
      "5220/5220 [==============================] - 21s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "history = rnn_cnn_model.fit(x=padded_train, \n",
    "                    y=y_train, \n",
    "                    validation_data=(padded_test, y_test), \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[checkpoint], \n",
    "                    epochs=epochs, \n",
    "                    verbose=1)\n",
    "\n",
    "y_pred_rnn_cnn = rnn_cnn_model.predict(padded_test, verbose=1, batch_size=2048)\n",
    "\n",
    "y_pred_rnn_cnn = pd.DataFrame(y_pred_rnn_cnn, columns=['prediction'])\n",
    "y_pred_rnn_cnn['prediction'] = y_pred_rnn_cnn['prediction'].map(lambda p: 1 if p >= 0.5 else 0)\n",
    "y_pred_rnn_cnn.to_csv('/Users/Ryan/Desktop/Programming/SMU/Machine Learning/y_pred_rnn_cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9256704980842911\n"
     ]
    }
   ],
   "source": [
    "y_pred_rnn_cnn = pd.read_csv('/Users/Ryan/Desktop/Programming/SMU/Machine Learning/y_pred_rnn_cnn.csv')\n",
    "print(accuracy_score(y_test, y_pred_rnn_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result is better than the CNN alone, but not as good as the RNN. In research I read, they achieved slightly better performance than an RNN with the CNN+RNN combo, but I was not able to replicate this.\n",
    "\n",
    "Some last minute research indicates that an LSTM is a useful layer to use in an RNN, so I'm going to implement that next and see how it does. LSTM stands for \"long short term memory\" and it is handles the vanishing gradient problem well, as it \"remembers\" for longer time steps than an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "lstm_out = 196\n",
    "\n",
    "inp = Input(shape=(MAX_LENGTH, ))\n",
    "x = Embedding(MAX_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=True)(inp)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "x = LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "lstm_rnn_model = Model(inputs = inp, outputs=outp)\n",
    "lstm_rnn_model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19700 samples, validate on 4925 samples\n",
      "Epoch 1/10\n",
      "19700/19700 [==============================] - 465s 24ms/step - loss: 0.3683 - acc: 0.8826 - val_loss: 0.3580 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.94193\n",
      "Epoch 2/10\n",
      "19700/19700 [==============================] - 554s 28ms/step - loss: 0.3476 - acc: 0.8829 - val_loss: 0.3092 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.94193\n",
      "Epoch 3/10\n",
      "19700/19700 [==============================] - 545s 28ms/step - loss: 0.2842 - acc: 0.8883 - val_loss: 0.2540 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.94193\n",
      "Epoch 4/10\n",
      "19700/19700 [==============================] - 497s 25ms/step - loss: 0.2130 - acc: 0.9150 - val_loss: 0.1889 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.94193\n",
      "Epoch 5/10\n",
      "19700/19700 [==============================] - 450s 23ms/step - loss: 0.1646 - acc: 0.9340 - val_loss: 0.1785 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.94193\n",
      "Epoch 6/10\n",
      "19700/19700 [==============================] - 450s 23ms/step - loss: 0.1295 - acc: 0.9468 - val_loss: 0.1919 - val_acc: 0.9275\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.94193\n",
      "Epoch 7/10\n",
      "19700/19700 [==============================] - 449s 23ms/step - loss: 0.1090 - acc: 0.9588 - val_loss: 0.2008 - val_acc: 0.9210\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.94193\n",
      "Epoch 8/10\n",
      "19700/19700 [==============================] - 448s 23ms/step - loss: 0.0889 - acc: 0.9668 - val_loss: 0.2112 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.94193\n",
      "Epoch 9/10\n",
      "19700/19700 [==============================] - 450s 23ms/step - loss: 0.0657 - acc: 0.9764 - val_loss: 0.2317 - val_acc: 0.9267\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.94193\n",
      "Epoch 10/10\n",
      "19700/19700 [==============================] - 450s 23ms/step - loss: 0.0492 - acc: 0.9813 - val_loss: 0.2586 - val_acc: 0.9261\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.94193\n",
      "4925/4925 [==============================] - 33s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "history = lstm_rnn_model.fit(x = padded_train, \n",
    "                               y = y_train, \n",
    "                               validation_data=(padded_test, y_test),\n",
    "                               batch_size=256, \n",
    "                               callbacks=[checkpoint], \n",
    "                               epochs=10, #5, 10, 15\n",
    "                               verbose=1)\n",
    "\n",
    "y_pred_lstm = lstm_rnn_model.predict(padded_test, verbose=1, batch_size=1024)\n",
    "\n",
    "y_pred_lstm = pd.DataFrame(y_pred_lstm, columns=['prediction'])\n",
    "y_pred_lstm['prediction'] = y_pred_lstm['prediction'].map(lambda p: 1 if p >= 0.5 else 0)\n",
    "y_pred_lstm.to_csv('/Users/Ryan/Desktop/Programming/SMU/Machine Learning/y_pred_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9260913705583756\n"
     ]
    }
   ],
   "source": [
    "y_pred_lstm = pd.read_csv('/Users/Ryan/Desktop/Programming/SMU/Machine Learning/y_pred_lstm.csv')\n",
    "print(accuracy_score(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model performed worse than the RNN, but better than the CNN and CNN+RNN combo. The model is very similar to the RNN model, with the addition of the LSTM layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was two-fold: to attempt to improve on a simple bag of words model using neural networks, and to gain expertise on neural networks. For the first goal, after extensive tuning and time spent running the neural networks, in this case they were not able to beat the bag of words model. I think a design change in the process made this easier than the previous work, in that there we classified to a score of 1-5, and here it was just a binary sentiment classification. This may give the simpler model the edge compared to more challenging problems where a neural network would perform better.\n",
    "\n",
    "Current research such as https://arxiv.org/abs/1808.03867 indicates that 2D CNNs perform better than other models for some language tasks, such as translation. That's similar to how I built the CNN model above, but it didn't perform. I should say that none of the models were bad, with each performing above 90% accuracy. It's just a question of more tuning. With more time, I think I could build a model that beats the bag of words.\n",
    "\n",
    "For the second goal, I think this has opened my mind to all the things I don't know about neural networks, particularly in the realm of NLP. I took this project on for more experience tuning and writing models, and while I got that, there's so much more to learn. This has given me great motivation to go forward and keep at it, as I am serious about machine learning using neural networks and NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
